\documentclass[main.tex]{subfiles}

\begin{document}

	\begingroup

	\renewcommand{\cleardoublepage}{}

	\renewcommand{\clearpage}{}

	\chapter{NLP Function Documentation}
		\chapterauthor{Merete Bommarius}	
	NLP has worked different sections of the field, starting from the beginning in the first Milestone. The group first made a list of potential commands the robot could receive in the previously mentioned challenges and then annotated them by hand to create a randomizer. This will be explained further in the following sections. This randomizer was then used to create thousands of commands which could be used to train the parser. So, it was NLP‘s job to train the parser so that the robot is able to understand what exactly it is that you want it to do, when you issue a more complex command involving one or more objects to act upon.\\ 
For the second Milestone, the attention of the group shifted from natural language processing to natural language generation. The shift in attention was caused by the completion of one section and moving on to a different one.\\ 
After the holidays, the project group as a whole got together and defined new goals for the second Milestone. They asked for hard coded commands like start and stop that would give the robot the signal to either start or stop a task or an action. Now, this technically counts as nlp, since the robot is processing language instead of generating it, but it was merely a side project. The main focus was on generating answers for the robot to give in certain situations, which will be elaborated later on.
		
	 \section{Natural Language Processing}
	 	\chapterauthor{Merete Bommarius}
	Natural language processing, short; NLP, describes the application of computational techniques to the analysis and synthesis of natural language and speech. It is currently used in the field of linguistics, computer science, information engineering and artificial intelligence (AI). Challenges with natural language processing involve the speech recognition itself, natural language understanding and the natural language generation.\\ 
The first challenge was to figure out whether shallow or deep semantic parsing should be used. Shallow semantic parsing is solely used for identifying entities in an utterance and labelling them with the roles they play. Therefore it is sometimes also referred to as slot-filling or frame semantic parsing. Deep semantic parsing on the other hand is concerned with producing precise representations of utterances that can contain significant compositionality. It is because of that also known as compositional semantic parsing.\\ 
As the name suggests, deep semantic parsing goes a lot more into detail when it comes to the parsing itself, but for our situation it would make the endresult a lot more unreliable. It would add a lot of information that isn‘t needed and therefore becomes unnecessary. So, it was decided to use shallow semantic parsing instead.

	\section{Natural Language Generation}
	 	\chapterauthor{Merete Bommarius}
	Natural-language generation (NLG) is a software process that transforms structured data into natural language. It can be used to produce automated reports for companies, as well as custom content for a web or mobile application. NLG is also used in well-known applications like Google or Amazon‘s Alexa.\\
Automated NLG can be compared to the process humans use when they turn ideas into writing or speech. This is referred to as language production.\\
To make the grammar more correct more easily, the current program imports a software called simplenlg. It helps with sentence structures and for example tenses and to distinguish between singular and plural.\\

	 \section{Implemented NLP Modules}
	 	\chapterauthor{Merete Bommarius}
	 	
	 	\begin{tabular}{ | p{3cm} | p{5cm} | p{5cm} | } 
	 		\hline 
	 		\textbf{Module} & \textbf{Communication} & \textbf{Description} \\ 
	 		\hline 
	 		Frame semantics & The frame semantics  module doesn‘t necessarily communicate with the other modules, but it is needed for training the parser. & It is used for background work to establish a working parser and provide a general structure that the parser can be trained with, when it‘s being trained on framing sentences correctly itself \\ 
	 		\hline 
	 		Context-Free grammar & The frame semantics  module doesn‘t necessarily communicate with the other modules, but it is needed for training the parser. & Again, the context-free grammar serves as background work, done beforehand. This time for the randomizer that generated the amount of commands needed for training the parser. \\
	 		\hline 
	 		Parsing & The parser communicates with the TinyRPC interface, which is used to make the communaction between Python2 \& Python3 possible. This then forwards the communicates with the Planning group of the SUTURO project, which is responsible for bringing everything together. & The parser is a way for the group to computationally annotate sentences, so that it‘s not needed to do it by hand anymore.\\
	 		\hline 
	 		Training the parser & This part is a subsection of the parsing module, which serves as further elaboration. & This is needed to make the parser more accurate in the fields that the group needs it in.\\
	 		\hline 
	 	\end{tabular} 

	
	\section{Frame Semantics}
	 	\chapterauthor{Merete Bommarius}
	Frame semantics is a theory of linguistic meaning, developed by Charles J. Fillmore. It extends his earlier case, grammar and relates linguistic semantics to encyclopedic knowledge. The theory applies the notion of a semantic frame, which is also used in AI. A semantic frame can also be defined as a coherent structure of related concepts. The frames themselves are based on recurring human experiences.\\
As mentioned in the previous section, frame semantics are used to give the parser a sense of structure. The way the group approached this was by framing about 100 commands by hand and then used those annotated sentences as examples of how the parser should interprete sentences that might be similar but are not in the list of commands. 
Here is an example of an annotated sentence: 

\texttt{ [bring]$_{action}$  [me]$_{beneficiary}$ [the pills]$_{item}$ [from the shelf]$_{source}.$}
	
	\section{Parsing}
	 	\chapterauthor{Merete Bommarius}
	As mentioned previously, shallow semantic parsing is used to parse and frame the commands that the robot will receive. The one that the group initially wanted to used is named open-sesame but is currently unavailable since it is being updated and rewritten to work with Python3. Instead we use a parser called Sling. It is not as good as open-sesame would have been, which is why it needs training.\\ 
In the first Milestone, the parsing for the sentence looks like this:

\begin{verbatim}
{‘predicate’: ‘take’} {‘ARG1’: ‘cup’} {‘ARG2’: ‘table’}
\end{verbatim}

The sentence used as an example for this was: ‘take the cup to the table‘. After training the parser, the annotation for the sentence would look more like:

\begin{verbatim}
{‘predicate’: ‘take’} {‘object’: ‘cup’} {‘destination’: ‘table’}
\end{verbatim}
	
	\section{Training the Parser}
	 	\chapterauthor{Merete Bommarius}
	In order to really be able to train the parser there are a few requirements that need to be met. First, there must be a corpus which the training program can use as a reference. This corpus should contain 50.000+ sentences. The solution that was used in order to get to this number was writing about 120 sentences and annotate them. Annotating helps setting the guidelines for the grammar that was used to set guidelines for the random sentence generator that was used to provide us with the required numbers of sentences.
	The grammar that was written looks for example like this:

		
		\texttt{MovementOnly\\
		MovementOnly $\rightarrow$ take ITEM [from SOURCE] [to DESTINATION] [MANNER]\\
		MovementOnly $\rightarrow$ bring [BENEFICIARY] ITEM [from SOURCE]\\}
		
		\texttt{This describes the 'MovementOnly' action.
		With this action, they primarily use their motion rather than their visual sense. This action contains simple movements like moving from a source to a\\ destination or simply into a given direction.}
		



	As you can see, the grammar starts off with a term which is then explained in further detail. It also provides a structure that is to be used for the sentence, which is later being randomly generated.\\ 
Depending on how thoroughly the training of the parser is supposed to be, it might take a few days up till a few weeks, and the parser needs to be retrained if new information or commands are added.

	\section{Generated Speech}
	 	\chapterauthor{Merete Bommarius}
The output the robot can give if it was given a task is split into six different sections. These sections are called; reject, task, waiting, failed, finished, and complications. Reject describes the decision the robot makes to deny the task, while task described the decision to perform it. Waiting and failed are similar. While waiting describes the robot waiting to understand the 
given task, failed describes that the robot has failed to understand. Finished means that the robot has finished performing the given task and complications describe, as the name says, complications that can occur during the performance of the task.\\ 
Outcomes for the different sections can for example be:
		
\begin{verbatim}
Reject = "Get someone else." or "Me is too superior for task."
Task = "Yes, Human." or "Me got Human covered."
Waiting = "Calibrating." or "Please wait."
Failed = "Please speak loud and clear." or "Please repeat."
Finished = "Mischief managed." or "Task completed."
Complications = "Humans are too tall, Me cannot see." or "Object is too small."
\end{verbatim}
	
	The reject section is solely for laughs and giggles. The robot will still perform the task.
	
	\section{Context-Free Grammar}
	 	\chapterauthor{Merete Bommarius}
	The NLP group works with something called context-free grammar (CFG). It is a certain type of formal grammar, meaning, a set of production ruled that describe all possible strings in a given language. Productions rules are simple replacements. For example there is a sentence like:

\begin{verbatim}
Bring the [item] to the [surface].
\end{verbatim}

Both [item] and [surface] are replacable and would count as a production rule. Now, this example is already more specific. A more vague example would look like this:

\begin{equation}
A \rightarrow \alpha
\end{equation}
\begin{equation}
A \rightarrow \beta
\end{equation}

‘A’ can be replaced by about anything given any value. To translate this into a more Suturo related context, the grammar would translate to this:

\begin{equation}
A \rightarrow [item]
\end{equation}
\begin{equation}
A \rightarrow [surface]
\end{equation}

Both item and surface can then be further defined. In CFG all rules are one-to-one, one-to-many, and one-to-none. These rules are applied regardless of context, hends the name context-free grammar. Another rule is, that the left side of th production rule must always be a nonterminal symbol, but for understanding purposes, this rule was ignored here.

	\section{Creating Grammar}
	 	\chapterauthor{Merete Bommarius}
	Another thing that was on NLP's task list was the phonetic spelling of the words that we have in our owl ontology. Those words are the items that the robot knows. It is not yet complete, but it was up to the group to make sure the robot can give accurate verbal output. The type of phonetic spelling that the speech program of the HSR uses is a modified version of ARPABET. SUTURO already had a very large dictionary containing a lot of words but not everything we needed was  in there, so I made a new one, containing the words of the ontology, which was hard to do at times since some items had names that consisted of more than one word, but where all in one word. Here is an example (the arrow isn‘t part of the original file, but I am using it to make things more
organized in the report):

\begin{verbatim}
ALETEGEMUESEKARTOFFELHUEHNERFLEISCH \rightarrow \\
AA L EH T EH G EH M IH S EH K AA R T AO F EH L HH IH N ER F L AY SH
\end{verbatim}
	\endgroup

\end{document}
