\documentclass[main.tex]{subfiles}
\begin{document}
	
	\chapter{Planning}
		\chapterauthor{Torge Olliges, Tom-Eric Lehmkuhl}
		
                The planning team is consisting of Torge Olliges (Groupleader), Philipp Klein (Clean up), Tom-Eric Lehmkuhl (Grocery storing) and Jan Schimpf (Clean up). The group is responsible for the integration and connection of the results of the other groups. To achieve this task the planning group orientated itself along the lines of the legacy code of our predecessors and came up with the general architecture of the project. The planning group was also mainly in charge of testing on the HSR. At the beginning of this milestone planning created diagrams which model how the robot has to behave, depending on the task (Clean up/Grocery storing). During the time that the group worked on the second milestone, planning communicated the progress and current tasks in the group so that everybody knew what the other group members were doing and how they were progressing. In the following sections (one for each package or abstraction layer) the concrete tasks and results of our work will explained and presented.
		
                \section{Low Level Interfaces (LLIF)}
	                The low level interfaces package mainly consist of the action client which connect to the action servers of the other groups.
                
                \subsection{Manipulation}
	                \begin{itemize}
						\item The /textbf {Grasp-action-client} and textbf {Place-action-client} were updated with an additional argument grasp pose this specifies \item from which direction the object is grasped.
						\item The /textbf {percieve-action-client} was renamed to /textbf {take-pose-action-client} to prevent further confusion between it and the perception clients.
						\item The /textbf {take-pose-action-client} was modified and now either takes an Integer for a predefined pose or it receives the value 0 too and then ten more Integers to move the HSR into a given pose.
					\end{itemize}
                \subsection{Navigation}
	                 \begin{itemize}
		 				 \item poi-client is an interface to a navigation client, which returns points in a room, where objects could be 
		  				 \item obstacle-map-subscriber is an interface to check if there is a point in the obstacle map
					\end{itemize}  
                \subsection{Knowledge}
	                \begin{itemize}
	 				      \item knowlegde-inserter is an interface to knowledge, which can insert the objects recognized by perception into the knowledge base
				    \end{itemize}
				          \textbf{knowledge-client} \\
				          The knowledge-client is an interface to knowledge, which is responsible for the request to the knowledgebase. For every request exists a function, which calls the prolog-query and returns the result. Implemented requests are:
				    \begin{itemize}
				             \item \textit{prolog-table-objects} returns all objects on the table
				             \item \textit{prolog-object-goal} returns the goal shelf for an object
				             \item \textit{prolog-object-goal-pose} returns the goal pose for an object
				             \item \textit{prolog-all-objects-in-shelf} returns all objects in the shelf
				             \item \textit{prolog-next-object} returns the next object to grasp choosen by knowledge
				             \item \textit{prolog-object-dimensions} returns the dimension for an object (depth, width, height)
				             \item \textit{prolog-object-pose} returns the pose of an object as list
				             \item \textit{prolog-table-pose} returns the pose of the table as list (x, y, z)
				             \item \textit{prolog-shelf-pose} returns the pose of the shelf as list (x, y, z)
				             \item \textit{prolog-object-in-gripper} returns the dimension of the object in gripper as list (depth, width, height)
				    \end{itemize} 
				    The functions \textit{knowledge-set-table-source} and \textit{knowledge-set-ground-source} are for setting the sources for selecting the next object. The task "Storing Groceries" needs the table as source and "Clean-up" needs also the ground as source. 
                \subsection{Perception}
	                For this milestone we integrated the action clients for the communication with perception. Namely the robosherlock object action client and the robosherlock plane action client.
	                \begin{itemize}
	                	\item The robosherlock object action client is responsible to maintain a connection to the perception action server, to call the robosherlock pipeline and to return the results, which consist of the detected objects within the region/regions which the \textit{call-robosherlock-object-action} function has as a parameter. This region or regions are then used to apply the region filters on the perception side.
	                	\item The robosherlock plane action client would be responsible to trigger the robosherlock plane action server and handle with it similarly to the robosherlock object action client but is not in use for this milestone because we decided at the beginning not to detect doors this milestone. But we are planning to put it to use in the robocup event. 
	                \end{itemize} 
                \subsection{NLP}
	                The \textbf{nlp-subscriber} provides functions for subscribing the topics from nlp. At the topic \textit{suturo\_speech\_recognition\/hard\_commands} the commands "start", "stop" and "continue" are published, if they are recognised by nlp. The subscriber for the hard-commands \textit{static-command-listener} calls the function \textit{set-state-fluent} with the message as argument and a fluent-variable is set according to the command. The message for dynmaic-commands is not defined yet.

                
                \section{Common Functions (COMF)}
	                The common functions package is the renamed common plans package and consists of functions which are common to both tasks. We created a functions file corresponding to the groups which they are interacting with and providing mid level functions to.
	                \subsection{Designators}
		                For this milestone we added designators to use in our high level plans. To be able to interact more abstractly with other groups in our execute files and high level plans.
                
		                We added a motion designator to be able to interact more abstractly with the movement of the HSR which wrapped the \textit{call-nav-action} function out of the LLIF package. This was for example used to create the move to table and move to shelf functions which navigate the robot to the shelf or table without having to directly use low level functions.
		                The designators \textit {grasping} and \textit {placing} are wrappers for \textit {call-grasp-action} and \textit {call-place-action} and both get 12 arguments the x,y,z coordinates of the object, the x,y,z,w quaternion values, the x,y,z dimensions of the object, the object-id and the grasp-pose. 
                
		                The designators \textit {saying} and \textit {perceive} are wrappers for \textit {call-text-to-speech-action} and \textit {call-robosherlock-pipeline}. 
                
	                \subsection{Manipulation}
		                Manipulation Functions:
		                grasp-object
		                place-object
		                place-object-list
		                create-place-list
                
                
                
	                    \begin{itemize}  
			              \item \textit{grasp-object} in \textbf{manipulation-functions} gets a object-id and the grasp-pose as arguments, uses the object-id querie knowledge for the pose of the object with \textit{prolog-object-pose}, the dimensions of the object with \textit{prolog-object-dimensions} and then uses the returned data to call the grasping designator.
		                  \item \textit{place-object} in \textbf{manipulation-functions} gets the object-id of the object in the gripper and which grasp-pose is used to grasp it as arguments, then knowledge is queried to get the goal pose of the object ith \textit{prolog-object-goal-pose}, the dimensions of the object with \textit{prolog-object-dimensions} of the object and then uses the returned data to call the placing designator.
		                  \item \textit{place-object-list} in \textbf{manipulation-functions}
		                  gets a list with arguments to call the placing designator 
		                  \item \textit{create-place-list} in \textbf{manipulation-functions} 
		                  gets the object-id of the object in the gripper and which grasp-pose is used to grasp it as arguments, then knowledge is queried to get the goal pose of the object ith \textit{prolog-object-goal-pose}, the dimensions of the object with \textit{prolog-object-dimensions} of the object, this is then used to create a list with five elements that can be used to call \textit{place-object-list}. Four of these elements are slightly offset on either the x-coordinate or the y-coordinate by 5cm.  
		                  \item \textit{place-hsr} in \textbf{high-level-plans}  gets the object-id of the object in the gripper and which grasp-pose is used to grasp it as arguments, makes use of \textit{create-place-list}, calls \textit{place-object-list} with the first element, if it fails then the first element is removed from the list and retried and loop until either we had a success or the list is empty
		                  \item \textit{grasp-hsr} in \textbf{high-level-plans} gets a object-id and the grasp-pose as arguments, it calls \textit{place-object} and if that fails it will get into position to percieve the object, call perception and then insert the new data into knowledge and then retry the call \textit{place-object} for the object.
		                  \item \textit{move-grasp} in \textbf{high-level-plans} gets a object-id and the grasp-pose as arguments, it moves the hsr into a position from where it can grasp and then calls \textit{grasp-hsr }.
                  
	                  \end{itemize}
                \subsection{Navigation}
                  \begin{itemize}
                    \item \textit{move-to-table} in \textbf{high-level-plans} queries the position of the table from knowledge, and adds a buffer, to create a position before the table and navigates the robot to it. If the argument is NIL, the robot will position himself frontally to the table. If the argument is set to T, the robot will be turned for percieving the table.
                    \item \textit{move-to-shelf} in \textbf{high-level-plans} queries the position of the shelf from knowledge, and adds a buffer, to create a position before the shelf and navigates the robot to it. If the argument is NIL, the robot will position himself frontally to the shelf. If the argument is set to T, the robot will be turned for perceiving the shelf.
                  \end{itemize}
                \subsection{Knowledge}
                \subsection{Perception}
	                \begin{itemize}
	                	\item  \textit{get-confident-objects} in \textbf{perception-functions} gets the message which is received by the robosherlock action client and filters the found objects for insufficient confidence. It checks first if the confidence\_class is higher then the threshold value 0.5 if that it is not the case then it checks the confidence\_shape and the confidence\_color if none of the values passes the check then the object is removed from the message.
	                	\item \textit{scan-shelf} in \textbf{perception-functions} will be used in the execute grocery file to get a cleaner and easier to maintain code. Functions similar to this which will combine a few function calls to make the usage easier will be added for grasping and placing in the future.
	                \end{itemize}
                \subsection{NLP}
                \section{Grocery Storing (GROCERY)}
                This package depends on common functions and low level interfacing and contains the execute file for the grocery task. Furthermore it contains a init file for the task. This connects our action clients out of the LLIF package to the action servers of the other groups and starts the master node for planning (planning\_node). The grocery\_bw file provides functions for the interaction with the bullet world.
                
                \section{Cleanup (CLEAN)}
                Equal to GROCERY just different inits and execute files.
                \section{Bullet World}
	                The bulletworld is a new feature which we implemented for this milestone. It is a representation of the real world which lets us check for visibilty or reachability without having to use the real robot. A function to spawn primitive objects in the bullet world has been added, which spawns objects that were detected by the robosherlock pipeline, this used to have a representation of the current world state. It was decided not to test grasping with the bullet world because giskard can't be used in the bullet world, this would minder giskards capabilities because giskard could grasp better then any script we could have used to emulate giskard in the bullet world. We test possible positions for the HSR with the bullet world if he can drive to the spot in the bullet world then the HSR can do it in the real world aswell. These possibilities are used in the high level plans file where we have implemented functions like \textit{move-to-shelf}, \textit{move-to-table}, \textit{move-hsr} which are explained above.
                
\end{document}
