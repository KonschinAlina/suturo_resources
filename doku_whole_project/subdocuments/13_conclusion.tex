\documentclass[main.tex]{subfiles}

\begin{document}

	\begingroup

	\renewcommand{\cleardoublepage}{}

	\renewcommand{\clearpage}{}

	\chapter{Conlusion}
		\chapterauthor{Jan-Frederik Stock}
		In the following chapter, the achievements and additions to previously available features and frameworks of the six subgroups of the SUTURO project will be described.
		
		\section{Perception}
		The perception groups objective was to use the available image data of the HSR to extract as much information as possible, for the other groups to use.
				
		Firstly the perception group made their robosherlockpipeline available through actionservers, so that it can easily be used by other parts of the system. Several annotators were added to perform new tasks, or improve upon previously available features. 
		
		Secondly, the SuturoShapeAnnotator was implemented, which offers improved shape detection over the annotator included in robosherlock. 
		
		Third, the clusterLabeling package was added, including the clusterLabelingAnnotator, which displays numbers for every cluster in a given frame, and the classificationEvaluatationAnnotator which calculates the accuracy of a classifier on a given scene, based on labeling data created with the help of the classificationEvaluationAnnotator. 
		
		Additionally, the rs\_hsrb\_perception package provided by previous SUTURO projects were adapted to work with the newest version of robosherlock. Also some changes were made to the package, including a change to the classification, and it was made possible to visualize the current pipeline.
		
		Another addition by the perceptiongroup was a classifier on 2D images, which uses Deep Learning. This could for example be used to recognize specific persons.
		
		Furthermore, a python pure embedding annotator was added, to be able to work on the images received by robosherlock via a python script.
		
		Perception also worked on handcameratracking, which could be used to correct the movement while grasping an object. A prototype implementation for this is available, but it could not be integrated due to versionconflicts. 
		
		An essential part of perception was also to record images to train classifiers on, a problem the group worked on in this context was the recording of transparent objects.
		
		Another package implemented by the perceptiongroup was the classificationEvaluation package, which can be used to save classification results in a markdown table and plot them.
		
		
		
		
		\section{Knowledge}
		The knowledge groups objective was to store information given to it by other groups, mainly perception, in an organized manner and making specific, needed information available.
		
		Knowledge reworked the underlying ontologies on a fundamental level and introduced the new concept of surfaces, which are now part of the ontologies.
		
		Furthermore, they can handle more features of objects they get into their knowledgebase, specifically they can handle every feature they are given by perception.
		
		They managed to reduce the amount of URDF-files for the entire project to one, making the URDF more easily manageable.  Also they introduced a new URDF xacro script to not only make it easier to adjust the associated yaml-file to new environments and scenes, but also make the resulting TF-Tree more intuitive.
		
		The surfaces are now generic and are organised according to their role and not their physics, which makes them more intuitively accessible. Knowledge is also able to handle any number of surfaces.
		
		Lastly, knowledge improved on the placing of objects, which now adapts better to the environment. 		
		\section{NLP}
		The NLP groups task was to both generate speech, so that the robot can give a vocal output, and process speech as an input, so that the robot can react to vocal commands.
		
		The NLP group managed to generate sentences, so that the robot can give a vocal output, describing, what its doing at a given moment.
		
		Also, spoken commands like "start" and "stop" can be understood, so that they can be used by planning to start and stop the robot at any time.
		
		\section{Navigation}	  	
		The navigation groups objective was to navigate the robot from one point to another in the world, without hitting any obstacles.
		
		To achieve their task, the navigation group recorded maps of the robotics lab, which were then provided to the robots navigation module. With this information, the robot can navigate through the lab without hitting obstacles.
		
		Furthermore, the navigation group developed a software that uses the robots laserscanner to detect possible objects standing on the ground, and marks them as points of interest, which can then be further explored.

		\section{Manipulation}
		The manipulation groups assignment was to interact with the environment by picking up and placing objects. They were responsible for moving the robots joints to perform specific tasks.
		
		Manipulation provided several actionservers to perform different tasks. Firstly they developed an actionserver that moves the robot into a position, from which the perception module can perceive objects. The second actionserver allows the user to move the robots gripper to a specific position. Furthermore, the provided an actionserver with which the user can grasp a specific object. The last actionserver can be used to place a previously grasped object.
		
		\section{Planning}
		The task of the planning group was to write plans for the two robocup tasks "clean up" and "storing groceries" which use the components of the other groups to reach the goals specified in these tasks.
		
		The planning group managed two write function plans for both tasks, including failure handling. Also Planning integrated hard commands like "start" and "stop" into their plans, which are voicecommands received by NLP.
	\endgroup

\end{document}
