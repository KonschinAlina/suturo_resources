\documentclass[main.tex]{subfiles}

\begin{document}

	\begingroup

	\renewcommand{\cleardoublepage}{}

	\renewcommand{\clearpage}{}

	\chapter{Architecture Overview}
	\chapterauthor{Torge Olliges,\\ Marc Stelter}
		In this chapter the architecture and interaction between the different components will be summarized and explained.
		In order to cope with the previously introduced challenges, the following five expert groups have been formed:

		\begin{itemize}
			\item Navigation: Movement and localization
			\item Perception: Recognition of objects and classification
			\item Knowledge: Storage and providing access to information about the world state and object positions
			\item NLP: Speech processing and generation
			\item Manipulation: Interaction with objects within the world
			\item Planning: Decision making, execution of plans and error handling
		\end{itemize}
		
		\section{Basic Architecture}
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=1.1\textwidth]{pictures/diagramms/overview.pdf}
			\caption{Architecture overview}
			\label{architecture}
		\end{figure}
		

			In the diagram \(\ref{architecture}\) the components each group was responsible for are visualized. The Perception component consists mainly of the \textit{SuturoProcessManager} module which implements a process manager to execute the \textit{RoboSherlock} pipeline. The \textit{RoboSherlock} pipeline then handles the classification and annotation. The results are collected and converted by the \textit{SuturoProcessManager}.  The Knowledge component consists of the Knowledge base, which is a wrapped information storage (database) for the HSR. It handles position data as well as some logic for goal position determination. The Manipulation component is responsible for gripper, head, body movement and grasp, place operations. For this Manipulation is using \textit{Giskard}. The Navigation component is only used for moving the HSR and for collision avoidance during movement and uses the basic movement interface of the HSR. The Planning component uses \textit{Cram} to implement multi layered plans for executing the tasks. It is split into different hierarchical levels; cleanup \ref{clean} and grocery \ref{grocery} being on the highest level (execution level). These packages contain plans which call functions from the mid level (common functions) \ref{comf}. The lowest level used for communication with other groups low level interfacing \ref{llif}.
			
			\begin{itemize}
				\item{\textbf{Cram}} \\
					 \textit{Cram} (Cognitive Robot Abstract Machine) is a software toolbox for the design, the implementation, and the deployment of cognition-enabled autonomous robots performing everyday manipulation activities.
				\item{\textbf{Giskard}} \\
					\textit{Giskard} is a  framework for constraint based motion planning and execution.
				\item{\textbf{Julius}} \\
					\textit{Julius} is a continuous speech recognition software. It is capable of recognizing spoken sentence as-live. Julius' main strength is the fact that it works on dictionaries. Dictionaries are a simple way for developers to adapt Julius to only recognize domain-specific sentences. Julius is not specifically created for ROS, but Toyota created a wrapper to allow the output of Julius to be published in ROS.
				\item{\textbf{KnowRob}} \\
				    \textit{KnowRob} is a framework created to combine different knowledge sources (such as knowledge about and provided by the robot, common sense, research etc.) and provide reasoning methods to work with them. 
				\item{\textbf{RoboSherlock}} \\
					\textit{RoboSherlock} is a common framework for cognitive perception, based on the principle of unstructured information management (UIM).
				\item{\textbf{Caffe}} \\
					\textit{Caffe} is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors.
			\end{itemize}
		
		\section{Interfaces}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=1.5\textwidth]{pictures/diagramms/rosgraph}
			\caption{Interfaces}
			\label{interfaces}
		\end{figure}

		In figure \ref{interfaces} the action servers and some other communication can be seen. As the main node is the \textit{planning\_node} which requests actions from other servers like the \textit{place\_server}, \textit{grasp\_server} or \textit{perception\_server} it is involved in all our communication. The sequence in which these calls are made will be explained in detail in chapter \ref{grocery-sequence} Storing Groceries Task Overview and chapter \ref{cleanup-sequence} Clean Up Task Overview. This section will give a short introduction to communication in ros and what messages were created for this project.

		\subsection{Form Of Communication}
		
			\subsubsection{Action Server}
			
			Action servers provide a synchronized form of communication integrated into the ROS environment. As this is a server based communication the connection between client and server has to be made. This connection can not fail during runtime so a connection leads most of the time to a failed execution. In this project action servers were mainly used because of the synchronism, this way if an action fails the server will send a corresponding result which can be caught and handled by the high level plans.
			
			\subsubsection{Topic Based}
			
			ROS also provides topic based communication which follows the listener/subscriber design pattern. It is a asynchronous way of communication where the result of the triggered action or request is not guaranteed. It is also not guaranteed that the subscriber received the message which was sent via a given topic. In this project topic based communication was used a few times in situations where it is not crucial to get an immediate result.  
		

		For our interfaces shown in \ref{interfaces} . 

		\subsection{Messages}
		\subsubsection{Object In Gripper}
		\begin{lstlisting}
		uint8 GRASPED=0
		uint8 PLACED=1
		
		string object_frame_id
		geometry_msgs/PoseStamped goal_pose
		uint8 mode
		\end{lstlisting}
		\subsubsection{Speech Generation}
		\begin{lstlisting}
		uint8 MOVE=0
		uint8 GRABING=1
		uint8 MOVEOBJECT=2
		uint8 PLACING=3
		
		uint8 FUTURE=50
		uint8 PRESENT=51
		uint8 PAST=52
		
		uint8 Type
		
		uint8 Tense
		
		SpeechgenerationAttribute[] Attributes
		\end{lstlisting}
		\subsubsection{Speech Generation Attribute}
		\begin{lstlisting}
		uint8 OBJECTNAME=0
		uint8 OBJECTID=1 # Use either Name or ID the name will be said directly, while the ID will be the representation from knowledge and processed later
		uint8 STARTSURFACE=2
		uint8 GOALSURFACE=3
		
		
		uint8 Type
		string Conntent
		\end{lstlisting}
		\subsubsection{Static Command}
		\begin{lstlisting}
		uint8 START=0
		uint8 STOP=1
		uint8 CONTINUE=2
		
		uint8 command
		
		\end{lstlisting}
		\subsection{Actions}
			\subsubsection{Move Base}
			\href{http://wiki.ros.org/move_base_msgs/MoveBaseAction}{ROS Reference}
			\subsubsection{Extract Object Info}
			This action is send when the \textit{RoboSherlock} should be triggered and when \textit{RoboSherlock} is done perceiving and returning its results.
			\begin{itemize}
				\item visualize - more visual output from \textit{RoboSherlock}
				\item regions - the regions which should be processed by \textit{RoboSherlock}
				\item detectionData - contains the objects detected by the \textit{RoboSherlock} pipeline
				\item
			\end{itemize}
			\begin{lstlisting}
			bool visualize
			string[] regions
			---
			ObjectDetectionData[] detectionData
			---
			string feedback
			\end{lstlisting}
			\subsubsection{Take Pose}
			\begin{lstlisting}
			uint8 FREE=0
			uint8 NEUTRAL=1
			uint8 LOOK_LOW=2
			uint8 LOOK_HIGH=3
			uint8 LOOK_FLOOR = 4
			uint8 GAZE=5
			
			uint8 pose_mode
			geometry_msgs/Vector3 gaze_point
			
			float32 head_pan_joint
			float32 head_tilt_joint
			float32 arm_lift_joint
			float32 arm_flex_joint
			float32 arm_roll_joint
			float32 wrist_flex_joint
			float32 wrist_roll_joint
			---
			uint8 SUCCESS=0
			uint8 FAILED=1
			uint8 error_code
			---
			float64 torso_joint_state
			\end{lstlisting}
			\subsubsection{Move Gripper}
			\begin{lstlisting}
			geometry_msgs/PoseStamped goal_pose
			---
			uint8 SUCCESS = 0
			uint8 FAILED = 1
			
			uint8 error_code
			---
			geometry_msgs/TransformStamped tf_gripper_to_goal goal frame
			\end{lstlisting}
			\subsubsection{Grasp}
			\begin{lstlisting}
			uint8 FREE=0
			uint8 FRONT=1
			uint8 TOP=2
			
			string object_frame_id
			geometry_msgs/PoseStamped goal_pose 
			geometry_msgs/Vector3 object_size
			uint8 grasp_mode
			---
			uint8 SUCCESS=0
			uint8 FAILED=1
			
			uint8 error_code
			---
			geometry_msgs/TransformStamped tf_gripper_to_object
			float64 gripper_joint_state
			\end{lstlisting}
			\subsubsection{Place}
			\begin{lstlisting}
			uint8 FREE=0
			uint8 FRONT=1
			uint8 TOP=2
	
			string object_frame_id
			geometry_msgs/PoseStamped goal_pose
			uint8 place_mode
			---
			uint8 SUCCESS=0
			uint8 FAILED=1
			uint8 error_code
			---
			geometry_msgs/TransformStamped tf_object_to_goal
			float64 gripper_joint_state
			\end{lstlisting}
			\subsubsection{Store Object Info}
			\begin{lstlisting}
			suturo_perception_msgs/ObjectDetectionData[] detectionData
			---
			bool succeeded
			---
			string feedback
			\end{lstlisting}
		

	  		 

	\endgroup

\end{document}
