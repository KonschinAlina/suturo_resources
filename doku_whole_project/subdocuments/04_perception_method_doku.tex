\documentclass[main.tex]{subfiles}


\begin{document}

\begingroup

\renewcommand{\cleardoublepage}{}

\renewcommand{\clearpage}{}
\newpage
\chapter{Perception Function Documentation}

\section{Introduction}
\chapterauthor{Jan-Frederik Stock}
The task of the perception group was to use the available image data to gather as much information as possible about the perceived scene. In the following chapter the tools and software used and created by the perception group will be described. These include descriptions of the pipelines and annotators used in RoboSherlock, the image processing framework used by the perception group, as well as tools for classification.

\section{RoboSherlock}
		\chapterauthor{Jan-Frederik Stock}
The main framework the perception group is working with is RoboSherlock \footnote{\url{http://www.robosherlock.org/}}. RoboSherlock is a software, that uses multiple expert systems called annotators on input image data. Each annotator performs a specific task, for example searching for objects in a given picture. All annotators write their results to a public data structure.\\
		
RoboSherlock works by executing all of these annotators in a predefined order. This order is called a pipeline. There can be pipelines for different purposes, in the perception groups case for detecting objects and detecting surfaces. Each pipeline can have different annotators. Thus, RoboSherlock can be customized to achieve many different image processing tasks.

		\section{rs\_perception}
		\chapterauthor{Leonidas Paniago}
To understand this chapter, the term pipeline needs to be further defined. The term pipeline in the informatics often refers to sequential processing of information.
In RoboSherlock, the pipeline fulfills a similar task. The processing of camera output is divided into several tasks that have to be executed one after the other.
Each task in the pipeline is called Annotator and it has an own purpose, here is a list of some tasks : 
		\begin{itemize}
			\item Annotation
			\item Detection
			\item Filter
			\item Flowcontrol
			\item Queryanswering
			\item Recognition 
			\item Segmentation
			\item Tracking
		\end{itemize}		
The pipeline order is determined by the already processed information and the needed information for the next step.
Each annotator contains changeable parameter that enables customization to the given tasks.
\href{https://github.com/SUTURO/suturo_perception/tree/master/rs_perception}{rs\_perception} includes all main pipelines. 

			\subsection{Pipeline: hsrb\_1ms}

The main pipeline for detecting and identifying objects with RoboSherlock consists of the following annotators in order:  
\begin{itemize}
	\item CollectionReader : Takes care of the Camera  Input
	\item ImagePreprocessor : Create missing Images and implement Image Filters  
		\begin{figure}[H]
   			 \centering
    			\includegraphics[width=0.5\linewidth]{pictures/2d/ImagePreProcessor.png}
   			 \caption{ImagePreprocessor}
  		\end{figure}
	\item SuturoRegionFilter : Remove all points that are not in the defined Regions 
		\begin{figure}[H]
   			 \centering
    			\includegraphics[width=0.5\textwidth]{pictures/pcl/RegionFilter.png}
   			 \caption{SuturoRegionFilter}
  		\end{figure}
	\item NormalEstimator : Estimate Surface Normals in a PointCloud 
		\begin{figure}[H]
   			 \centering
    			 \subfigure[NormalEstimator]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/2d/NormalEstimator.png}
			 }%
       			 \subfigure[NormalEstimator]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/pcl/NormalEstimator.png}
       			 }
   			 \caption{NormalEstimator}
  		\end{figure}
	\item PlaneAnnotator : Search for Horizontal Planes in which the search location for Objects is set
		\begin{figure}[H]
   			 \centering
    			 \subfigure[PlaneAnnotator]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/2d/PlaneAnnotator.png}
        			 }%
       			 \subfigure[PlaneAnnotator]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/pcl/PlaneAnnotator.png}
        			 }
   			 \caption{PlaneAnnotator}
  		\end{figure}
	\item PointCloudClusterExtractor : Extract all Points found in the PointCloud that are perpendicular to the Plane 
		\begin{figure}[H]
   			 \centering
    			 \subfigure[PointCloudClusterExtractor]{%
            		\includegraphics[width=0.5\textwidth]{pictures/2d/PointCloudClusterExtractor.png}
        			}%
        			\subfigure[PointCloudClusterExtractor]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/pcl/PointCloudClusterExtractor.png}
        			}
   			 \caption{PointCloudClusterExtractor}
  		\end{figure}
	\item ClusterColorHistogramCalculator : Calculate the individual color quantity in relation to all colors on the cluster   
		\begin{figure}[H]
   			 \centering
    			\includegraphics[width=0.5\textwidth]{pictures/2d/ClusterColorHistogramAnnotator.png}
   			 \caption{ClusterColorHistogramCalculator}
  		\end{figure}
	\item Cluster3DGeometryAnnotator : Extracts basic attributes of cluster like initial pose, semantic size and 3D bounding box 
		\begin{figure}[H]
   			 \centering
    			 \subfigure[Cluster3DGeometryAnnotator]{%
            		\includegraphics[width=0.5\textwidth]{pictures/2d/Cluster3DGeometryAnnotator.png}
        			}%
        			\subfigure[Cluster3DGeometryAnnotator]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/pcl/Cluster3DGeometryAnnotator.png}
        			}
    			 
   			 \caption{Cluster3DGeometryAnnotator}
  		\end{figure}
	\item SuturoShapeAnnotator : Detects the shape of the object (cylinder, sphere, box)
		\begin{figure}[H]
   			 \centering
    			 \includegraphics[width=0.5\textwidth]{pictures/2d/SuturoShapeAnnotator.png}
   			 \caption{SuturoShapeAnnotator}
  		\end{figure}
	\item RegionAnnotator : Further restriction on which areas will be used based on defined Regions 
	\item CaffeAnnotator : Deep learning method for Object recognition based on prerecorded Images 
	\item KnnAnnotator : Uses K-Nearest-Neighbor for classification of Objects 
		\begin{figure}[H]
   			 \centering
    			 \includegraphics[width=0.5\textwidth]{pictures/2d/KnnAnnotator.png}
   			 \caption{KnnAnnotator}
  		\end{figure}
\end{itemize}



			\subsection{Pipeline: hsrb\_planes} 
This pipeline is used to detect a shelf door. It is used to find a plane that is parallel to the z-axis of the camera.
\begin{itemize}
	\item CollectionReader : Takes care of the camera input
	\item ImagePreprocessor : Implements image filters  
	\item PointCloudFilter : Filters points out of the cloud that are not in the range of the given parameters for X, Y and Z axes
	\item NormalEstimator : Estimate surface normal's in a PointCloud 
	\item PlaneAnnotator : Finds a plane in the current scene and saves it into the CAS.
	\item PointCloudClusterExtractor : Extracts all points that are found in the PointCloud that are perpendicular to the plane 
\end{itemize}

			\subsection{Pipeline: storage\_suturo} 
Pipeline for storing camera images in MongoDB, all saved scenes can be replayed for testing without the HSR and have the same functionality. 
\begin{itemize}
	\item CollectionReader : Takes care of the camera input
	\item ImagePreprocessor : Implements image filters 
	\item StorageWriterSuturo :  Records defined camera topics into the MongoDB
\end{itemize}

		\section{actionserver}
		\chapterauthor{Leonidas Paniago}
The \href{https://github.com/SUTURO/suturo_perception/tree/master/actionserver}{actionserver} contains the server responsible for the pipeline execution they are based on the \href{http://wiki.ros.org/actionlib}{ROS Client-Server Interaction} and it works by providing three callable actions types:
\begin{itemize}
	\item Goal : A specific defined task 
	\item Feedback : Information about the goal execution  
	\item Result : The goal final result 
\end{itemize}
This approach facilitates the communication with perception by offering a clear interface.
Each server provides the same actions with different results based on their functionality.
The implemented clients are not needed for the HSR execution than knowledge has its own implementation, they are mainly used for testing and troubleshooting. 
Both servers give the same message output \href{https://github.com/SUTURO/suturo_resources/blob/master/messages/suturo_perception_msgs/msg/ObjectDetectionData.msg}{ObjectDetectionData.msg} consisting of : 

	\begin{itemize}
	\item Class
	\item Shape
	\item Colour
	\item Position
	\item Height
	\item Depth
	\item Width
	\item Orientation
	\end{itemize}

			\subsection{ExtractObjectInfoClient}
Call ExtractObjectInfoServer and wait until the result arrives or the time out runs out. Region can be set to limit the perception of objects to a specific region, if default is set the RegionFilter will be disabled. The client can be started by calling : \texttt{rosrun actionserver ExtractObjectInfoClient}

			\subsection{ExtractObjectInfoServer}
Receives a goal from Client and pass on to SuturoProcessManager, checks for results and publishes them under \texttt{perception\_actionserver/result}. 
			\begin{figure}[H]
   			 \centering
    			 \includegraphics[width=1\textwidth]{pictures/perception/suturo_ExtractObjectInfoServer.png}
   			 \caption{ExtractObjectInfoServer}
  			\end{figure}

			\subsection{ExtractPlaneInfoClient}
Call ExtractPlaneInfoServer and wait until the result arrives or the time out runs out. This client has no additional parameters.
The client can be started by calling : \texttt{rosrun actionserver ExtractPlaneInfoClient}.

			\subsection{ExtractPlaneInfoServer}
Receives a goal from Client and pass on to SuturoProcessManager, checks for result and publishes it under \texttt{perception\_actionserver\_plane/result}.
			\begin{figure}[H]
   			 \centering
    			 \includegraphics[width=1\textwidth]{pictures/perception/suturo_ExtractPlaneInfoServer.png}
   			 \caption{ExtractPlaneInfoServer}
  			\end{figure}

			\subsection{perception\_server}
perception\_server is the only way to launch \texttt{ExtractObjectInfoServer} and \texttt{ExtractPlaneInfoServer} they cannot be used separated. 
It works by calling :\texttt{ roslaunch rs\_perception hsrb\_perception.launch} in the rs\_perception directory.

		\section{rs\_turn\_table}
		\chapterauthor{Leonidas Paniago}
\href{https://github.com/Vanessa-rin/rs_turn_table}{rs\_turn\_table} is a package from the last Suturo group. There were no big changes to the core code, only small situational changes not implemented on the master. 
This package is mainly for image recording, all recorded images will then be used for object recognition.  
rs\_turn\_table heavily depends on Openni2 as driver for the kinect camera. HSR could be used for the image recording but in reality it turns out to be really impractical. 
The kinect camera offers more flexibility and frees the robot for other groups. Openni2 installation was a major problem in the project due to a lack of documentation. For this purpose there is a 
\href{https://github.com/SUTURO/suturo_perception/blob/Openni2/Openni2/Openni2_Install}{script} that will install all necessary dependencies and automatically enable depth\_registration in the Openni2 launch file. \\ Note: Openni2 did not work on Virtual machine with Ubuntu-16.04.6. The camera goes on but no output are sent to the camera topic. 

			\subsection{Pipeline: estimate\_plane}
\begin{itemize}
	\item CollectionReader : Takes care of the camera input
	\item ImagePreprocessor : Implements image filters  
	\item PointCloudFilter : Filters points out of the cloud that are not in the range of the given parameters for X, Y and Z axes
	\item PlaneAnnotator : Finds a plane and saves it to a file 
\end{itemize}
This Pipeline will save all visible planes, they are callable again. The best way to get a clean plane is to set the Limits on PointCloudFilter by hand. Only one plane should be visible. 

			\subsection{Pipeline: save\_images}
\begin{itemize}
	\item CollectionReader : Takes care of the camera input
	\item ImagePreprocessor : Implements image filters  
	\item PointCloudFilter : Filters points out of the cloud that are not in the range of the given parameters for X, Y and Z axes
	\item NormalEstimator : Estimate surface normal's in a PointCloud 
	\item PlaneAnnotator : Finds a plane in the current scene and saves it into the CAS
	\item PointCloudClusterExtractor : Extracts all points that are found in the PointCloud that are perpendicular to the plane 
	\item SaveClusterCloudsAndImages : Save Information about the Cluster to a file 
\end{itemize}
Uses the saved plane and takes periodic images from objects on the plane, if there is more than one cluster visible the annotator will not trigger the saving process.

		\begin{figure}[H]
   			 \centering
    			 \subfigure[Original]{%
            		\includegraphics[width=0.5\textwidth]{pictures/perception/turn_table_2.png}
        			}%
        			\subfigure[Pointcloud representation]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/perception/turn_table_1.png}
        			}
   			 \caption{Turn table setup}
			  \label{turnTableOriginal}
  		\end{figure}
In the pictures shown above the RGB texture and the pointcloud representation do not match fully. This occurs if the unregistered image topic is used for the pipeline. 

		\begin{figure}[H]
   			 \centering
    			 \subfigure[RGB]{%
            		\includegraphics[width=0.5\textwidth]{pictures/perception/Feurich_Stapel_Chips_Paprika_0_4_crop.png}
        			}%
        			\subfigure[Pointcloud representation]{%
           		 \includegraphics[width=0.5\textwidth]{pictures/perception/Feurich_Stapel_Chips_Paprika_0_3_mask.png}
        			}
   			 \caption{save\_image output}
  		\end{figure}

\section{classificationEvaluation Package}
\chapterauthor{Jan-Frederik Stock}

The classificationEvaluation package is used to save and display the classificationresults of the perception pipeline. The results can be saved in a markdown table and/or be displayed as a scatterplot. It consists mainly of the rsClassificationEvaluator.py python script. Here, the class ResultActionClient is implemented, which serves as an action client for the perception object info actionserver.\\

\subsection{The ResultActionClient class}
The class provides the following methods:

\begin{itemize}
\item 
sendGoal(self)\\
This method is used to send a goal to the ExtractObjectInfoServer and thus triggering the pipeline.

\item saveResult(self, result)\\
This method saves the actionresult, which is the classification, in a field of the class.

\item saveToMd(self, algorithm)\\
This method saves the collected results of the classification in a markdown table. If a markdown file with the name "evaluation.md" is found in the directory the script is executed in, the results are appended to it, otherwise, the file is created.

\item plotResult(self)\\
The method plots the collected results using the scatterplot function from matplotlib.
\end{itemize}

\subsection{Main method}
The main method tries to create an instance of the resultActionClient class, does some i/o with the user, sets the parameters for the resultActionClass, and then triggers the action. It also uses the provided methods to save the results to a markdown file and plot them.

\section{Feature Extraction Tools}
 \chapterauthor{Jan-Frederik Stock}
Perception uses feature extraction to get quantifiable data from the perceived images. On this data, the classification algorithms can then be trained. Every time a new image is perceived, its features are extracted, which are then used to classify it. Perception uses a neural network without the output layer to extract features, the output of this network are the features. The neural networking framework used for this is Caffe, the trained network is the BVLC Reference Net. The tools directory in the featureExtraction package contains the two bash scripts make\_split\_from\_directory.sh and make\_split\_list\_from\_directory.sh.

\subsection{make\_split\_from\_directory.sh} 
This bash script can be used to create a split YAML-file which is required to perform feature extraction with the rs\_addons package. To use it, one has to place the file in the parent directory of the directory, in which the folders with the recorded images are stored. When being executed, the script asks for a filename and the name of the directory out of which the YAML-file will be created. It then creates the YAML-file using the names of the directories in which the images are stored as class names.

\subsection{make\_list\_from\_directory.sh}
This bash script does nearly the same thing as the make\_split\_from\_directory.sh, except that it does not create a split file but a list of the class names. This is required by the classifying annotator and has to be pasted into the annotators YAML-file. The comma after the last class name in the list has to be removed manually.

\section{calculateConfidence in RSKNN.cpp}
\chapterauthor{Jan-Frederik Stock}
In the RSKNN.cpp, implementing KNN-classification in the rs\_addons package, the following method was added:

\begin{lstlisting}
double RSKNN::calculateConfidence(double classificationResult, cv::Mat neighborResponses)
\end{lstlisting}

It calculates the classification confidence for the KNN-classification by dividing the number of neighbors belonging to the result class, by the total number of visited neighbors. 

\section{The clusterLabeling Package}
\chapterauthor{Jan-Frederik Stock}
\subsection{Motivation}
The clusterLabeling package fixes a fundamental problem: Before its existence it was not possible to compare the performance of classifiers objectively. A user could only look at a given classification and decide if the algorithm was working good or bad overall, but not decide on a quantitative mesurement. The ability to do this is not just important to evaluate the perfomance of a single algorithm, but even more to compare the performance of different algorithms and select the best one for a specific use case.\\

A very common procedure to do this in machine learning is the evaluation of an algorithm by splitting the available data into a test set and a training set. The available data contains a class label for every sample. The algorithm is trained on the training set and the test set is held back to test the performance. This is done by introducing samples from the test set into the algorithm and then checking if the algorithm produces the correct label. By dividing the number of correctly classified samples by the total number of tested samples, the accuracy is calculated. This is a very common measure of classification performance and it is suitable for a multi class problem like this.\\

In the particular case of the SUTURO project, it was not practical to split a test set from our training data, because we recorded the objects by taking pictures all around. Taking some of these pictures away for testing would mean testing the classification on perspectives that are not represented in the training data, leading to worse classification performance. Also, in the real life application on the robot, the classification has to work on different scenes than the setting of the recording, so it makes sense to test the classfication on similar, more realisitic scenes. The ability to do this is implemented in the clusterLabeling package. The user is able to test their classification algorithm on scenes recorded on the actual robot they are working with.  

\subsection{Usage}

\subsubsection{The clusterLabeler}
The clusterLabeler is the Annotator used for labeling. It is best placed at the end of your labeling pipeline, which also has to contain the annotators necessary for clustering. The labeling and the evaluation work best if a good clustering is achieved, so it is recommended to use the PointCloudFilter or other filters you may have to ensure this. You mustn't change the settings of any annotator in the pipeline while creating the labels and using them for the evaluation later, otherwise, it can not be guaranteed that the cluster numbering will stay the same. To be able to label a scene frame by frame, put the Trigger at the beginning of your pipeline. You can use the classificationEval pipeline in the rs\_perception package to start.\\

To label a scene, you start your labeling pipeline and call the trigger service once. You can then chose the clusterLabeler in the visualizer, which will display a number for every cluster. You can see an example of how this looks in figure \ref{fig:clusterLabeler visualization}.
\begin{figure}
  \includegraphics[width=\linewidth]{pictures/perception/cluster_numbering.png}
  \caption{The visual output of the clusterLabeler.}
  \label{fig:clusterLabeler visualization}
\end{figure}

\newpage


Now, you can note the class for every cluster in a labeling JSON-file, which has to look like figure \ref{fig:labeling file}. You can also have look at the file already present in the labeling directory in the custerLabeling package for the correct syntax. Now call the trigger service again and proceed to label. If you do not want to create labels for a frame, for example if the clustering is bad, you can just leave the clusters array in the JSON-file empty. The classificationEvaluationAnnotator will leave this frame out of the calculation. Currently, one has to create the labeling JSON-file by hand, but there is a started implementation for a GUI for this task in the clusterLabeling package.

\begin{figure}
  \includegraphics[width=\linewidth]{pictures/perception/labeling_file.png}
  \caption{The JSON-file containing the labels.}
  \label{fig:labeling file}
\end{figure}


\subsubsection{classificationEvaluationAnnotator}
The classifcationEvaluationAnnotator can read a labeling file and will then compare the labels with the actual classification to calculate the accuracy. Swap the classificationEvaluationAnnotator with the clusterLabelingAnnotator and add you classifying annotators before it. Do not change anything else about the pipeline. Set the path to your labeling file in the descriptor YAML-file of the classificationEvaluationAnnotator and start your pipeline. Trigger the pipeline as many times as you have labeled frames in your JSON-File. The classificationEvaluationAnnotator will display the current accuracy in your step in \textit{RoboSherlocks} terminal output. This is shown in figure \ref{fig:classificationEvaluationAnnotator output}.

\begin{figure}
  \includegraphics[width=\linewidth]{pictures/perception/accuracy.png}
  \caption{The output of the classificationEvaluationAnnotator.}
  \label{fig:classificationEvaluationAnnotator output}
\end{figure}

\subsection{Documentation}
\subsubsection{clusterLabeler}
\begin{lstlisting}
TyErrorId processWithLock(CAS \&tcas, ResultSpecification const \&res_spec)
\end{lstlisting}
This is the main processing method of the annotator.\\

Firstly, it gets the detected clusters from RoboSherlocks central data structure like this: \label{getting the detected clusters.}

\begin{lstlisting}
//Get the clusters in the image found by previous annotators
std::vector<rs::ObjectHypothesis> clusters;
scene.identifiables.filter(clusters);
\end{lstlisting}

It then iterates over the found clusters and draws a rectangle around every found cluster with a number next to it, corresponding to its position in the clusters vector. If the filtering settings are not changed for the pipeline, this order does not change.

\begin{lstlisting}
//set region of interest for the cluster
rs::ImageROI cluster_roi = cluster->rois.get();
//create Rectangle
cv::Rect rectangle;
rs::conversion::from(cluster_roi.roi.get(), rectangle);
//Draw the numbering for the cluster
drawCluster(current_image, rectangle, std::to_string((cluster - clusters.begin())));
\end{lstlisting}

\begin{lstlisting}
static void drawCluster(cv::Mat input, cv::Rect rectangle, const std::string &label)
\end{lstlisting}

This is a helper function that creates the rectangle for the picture and inserts it into the matrix holding the picture of the scene, as well as the number:

\begin{lstlisting}
cv::putText(input, text, cv::Point(rectangle.x + rectangle.width, rectangle.y - offset - textSize.height), cv::FONT_HERSHEY_PLAIN, 1.5, CV_RGB(0, 0, 0), 2.0);
\end{lstlisting}

This will then be displayed in RoboSherlocks viewer.



\subsubsection{classificationEvaluationAnnotator}
The annotator class contains fields holding the path to the labeling file, a rapidjson document, which will later contain the object representation of the parsed JSON-file and fields used to calculate the accuracy.

\begin{lstlisting}
TyErrorId initialize(AnnotatorContext \&ctx)
\end{lstlisting}

This method is executed at the startup of the pipeline.  It gets the path to the labeling file from the YAML description and reads the file. The file is then parsed into a JSON object representation by rapidjson and saved in a field of the class, like so:

\begin{lstlisting}
document.Parse(content.c_str());
\end{lstlisting}

\begin{lstlisting}
TyErrorId process(CAS \&tcas, ResultSpecification const \&res_spec)
\end{lstlisting}

This is the main processing method of the annotator. It gets the identified clusters from the central RoboSherlock data structure like the clusterLabeler \ref{getting the detected clusters.}. It then checks, if the JSON-file has an entry for the current frame: 

\begin{lstlisting}
if (!document["clusterLabeling"][framecounter]["clusters"].Empty())
\end{lstlisting}

This allows for frames that have bad clustering to be skipped for the evaluation, as a good clustering is a prerequisite for the classification. If the JSON-file contains a labeling for the current frame, the method loops through the found clusters. In the loop, it first gets the classification from the algorithm under evaluation from RoboSherlocks central data strucutre:

\begin{lstlisting}
std::vector<rs::Classification> classResult;
clusters[i].annotations.filter(classResult);
\end{lstlisting}

If this result is not empty, the method compares it to the label from the JSON-file:

\begin{lstlisting}
if (classResult[0].classname.get().compare(
                          document["clusterLabeling"][framecounter]["clusters"][i]["label"].GetString()) == 0)
\end{lstlisting}

The classification result can be empty, if the classification algorithm filters out results not passing a certain confidence threshold. This is counted as an incorrect classification.\\

If the classes match, the number of correctly classified samples is incremented, as well as the total number of samples. If not, just the total number of samples is incremented.\\

After the loop, the accuracy at the current point of the calculation is calculated by dividing the number of correctly classified samples by the total number of samples:

\begin{lstlisting}
accuracy = ((float) correct_samples) / ((float) total_samples);
\end{lstlisting}

At the end, the framecounter is incremented and the calculation starts again for the next frame.

\section{rs\_hsrb\_perception Package}
\chapterauthor{Evan Kapitzke}

The \texttt{rs\_hsrb\_perception} package was created in the Suturo Master-project. The package implements a RoboSherlock process manager, called the \texttt{SuturoProcessManager}. Additionally, it contains modified annotators to annotate regions.

\subsection{RegionAnnotator}
\begin{itemize}
\item TyErrorId process(tcas, res\_spec)\\
Iterates through the clusters that are currently saved in the CAS and annotates the region in which they are presently based on the cluster pose.
This method only got changed to work with the \textit{RoboSherlock} master.
\end{itemize}

\subsection{SaveClusterCloudsAndImages}
Currently not used.

\subsection{SuturoClassifier}
Currently not used.

\subsection{SuturoProcessManager}
\begin{itemize}
\item setup()\\
Creates the ressource manager and instantiates the region vector.

\item init(pipeline)\\
Initializes the given pipeline (aggregate analysis engine YAML descriptor) and starts the visualizer.
This method got changed to allow the visualization of the pipeline.

\item run(args, detectionData)\\
Executes the initialized pipeline once with the given arguments and saves the result in detectionData.
This method now checks for the region "robocup\_default" and disables the region filter to perceive objects that can not be found
in a region. For example, a Pringles can that is placed on the ground.

\item has\_vertical\_plane()\\
Returns true if the current CAS contains a vertical plane.

\item getClusterFeatures(cluster, data)\\
Extracts the annotations of the given cluster and generates an ObjectDetectionData object. 
This object will be added to the data pointer. makeObjectDetectionData(...) from suturo\_conversion is used.
Currently only one KnnAnnotator (for known objects) is used to determine the class label of an object.

\item visControlCallback(req, res)\\
Callback for navigation through the different annotation previews in the visualization window.
\end{itemize}

\subsection{SuturoRegionFilter}
Adapted region filter from RoboSherlock.

\begin{itemize}
\item reconfigure()\\
Reconfigures the annotator to disable/enable its functionality without restarting the pipeline or removing it.
\end{itemize}

\subsection{suturo\_conversion}
Implements template functions to convert from and to PoseStamped objects.
Following functions got changed:

\begin{itemize}
\item decode\_shape(shapes)\\
Converts the result of the PrimitiveShapeAnnotator to match the required shape format in the ObjectDetectionData object.

\item makeObjectDetectionData(pose, geometry, shape, objClass, confidence, c, odd)\\
Combines the given parameters to an ObjectDetectionData message.
\end{itemize}

			\section{rs\_Athene}
			\chapterauthor{Leonidas Paniago De Oliveira Neto}
\href{https://github.com/SUTURO/suturo_perception/tree/Handcamera_tracking}{Handcamera\_tracking} is the GitHub sub-branch containing rs\_Athene and all following featured annotators. 
None of them made it to the main branch because they were not finished or were not suitable for any current task. \\
Each annotator can be executed by enabling the corresponding annotator in the descriptor athene.yaml and running : \\
\texttt{rosrun robosherlock run \_ae:=athene}. 

				\subsection{TwoDAnnotator}
The \href{https://github.com/SUTURO/suturo_perception/blob/Handcamera_tracking/rs_Athene/src/TwoDAnnotator.cpp}{TwoDAnnotator} is responsible for shape recognition that should replace the old ShapeAnnotator by improving the accuracy, reducing processing time, and expanding the usability. 
Since the old annotator only processes RGB-D images (color and dense depth images) which makes the head-camera the only usable camera for the task. The task also takes a very long time with less than desirable results. \\
To fix these issues, the TwoDAnnotator uses only RGB (color images) images, which makes the processing less demanding than there's fewer data to process. Furthermore, all the HSR cameras are usable for shape recognition because each one is capable of outputting RGB images. \\
The TwoDAnnotator works by converting the RGB image to grayscale :
\begin{lstlisting}
..
// Get RGB image and copy it to src
cas.get(VIEW_COLOR_IMAGE, src); 
..
//converting the original image into grayscale
cv.cvtColor(src, src_gray, CV_BGR2GRAY);
..
\end{lstlisting}
Then smoothing the grayscale output for better results : 
\begin{lstlisting}
..
//thresholding the grayscale image to get better results
cv.threshold(src_gray, src_gray, thresh, maxval, type);
..
//smooth the original image using Gaussian kernel to remove noise
cv.GaussianBlur(src_gray, src_gray, Size(9, 9), 0);
..
\end{lstlisting}
The output is used for edge detection by running the canny-algorithm : 
\begin{lstlisting}
..
// Detect edges using canny
cv.Canny(src_gray, canny_output, thresh, thresh * 2, 3);
..
\end{lstlisting}
All found edges are then used for the contour analysis : 
\begin{lstlisting}
..
//finding all contours in the image
cv.findContours(canny_output, contours, hierarchy, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE, Point(0, 0));
..
\end{lstlisting}
Each contour get check for the numbers of edges. By three edges it is assumed that it is a triangle, four edges implies that it is a square. In addition, each contour needs to reach a certain area threshold. The triangle area get checked using the Hero´s formula, squares using Bretschneider formula. \\
\begin{lstlisting}
..
//triangle area
double area = sqrt((a+b+c)*(a+b-c)*(b+c-a)*(c+a-b))/4;
..
// square area
double area = sqrt((4*pow(e,2)*pow(f,2)) - pow((pow(b,2) + pow(d,2) - pow(a,2) - pow(c,2)),2))/4;
..
\end{lstlisting}

Circles are are treated separately using Hough Circle Transform on the grayscale image : 

\begin{lstlisting}
..
//Circles Detection with Hough Circle Transform
vector<Vec3f> circles;
cv.HoughCircles( src_gray, circles, CV_HOUGH_GRADIENT, 1, src_gray.rows/8, 200, 100, 0, 0 );
..
\end{lstlisting}

The TwoDAnnotator utilize many functions from the openCV library, they can be recognized by the cv. prefix. \\ \\
During testing, it turns out that the Hough Circle Transform used to find circles is in the current form to slow. The whole annotator is way too unstable to be used in a real environment. It did not bring any improvement over the old ShapeAnnotator and need further work to be usable. Some form of edge smoothing and a better way to classify triangles and squares could bring better result but in the end, this approach was no longer needed and was discarded.
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{pictures/perception/TwoDAnnotator.png}
\caption{TwoDAnnotator output example}
\end{figure}
				\subsection{YOLO}
The \href{https://github.com/SUTURO/suturo_perception/blob/Handcamera_tracking/rs_Athene/src/Yolo.cpp}{YOLO} annotator is as the name suggests, an implementation for the You Only Look Once network. It contains all the needed code for the network to work.
But due to ROS not supporting openCV latest version there is no way to execute the annotator. It can be said that the algorithm will work if the supported version of ROS gets updated. Then a similar approach was utilized in the GoogLeNet annotator which succeeded and the created YOLO prototype worked like intended.
The approach is based on a python example and works by loading a pre-trained network : 
\begin{lstlisting}
..
// Load names of classes
ifstream ifs(classesFile.c_str());
string line;
while (getline(ifs, line)) classes.push_back(line);
..
// Load the network
Net net = readNetFromDarknet(modelConfiguration, modelWeights);
net.setPreferableBackend(DNN_BACKEND_DEFAULT);
net.setPreferableTarget(DNN_TARGET_CPU);
..
\end{lstlisting}
And feeding it with RGB images : 
\begin{lstlisting}
..
// get frame from the video
cas.get(VIEW_COLOR_IMAGE, frame);
..
// Create a 4D blob from a frame.
blob = blobFromImage(frame, 1/255.0, cvSize(inpWidth, inpHeight), Scalar(0,0,0), true, false);
        
//Sets the input to the network
net.setInput(blob);
..
net.forward(outs, getOutputsNames(net));
\end{lstlisting}

The output consists of all found classes and their bounding boxes that are over a set threshold.
In addition to classification the prototype calculated the distance from each bounding box center to the image center. 
Then the main focus for this annotator was, getting feedback on the current object position in relation to the camera center.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{pictures/perception/YOLO.png}
\caption{Python prototype output}
\end{figure}

				\subsection{GoogLeNet}
\href{https://github.com/SUTURO/suturo_perception/blob/Handcamera_tracking/rs_Athene/src/GoogLeNet.cpp}{GoogLeNet} is an annotator with a similar approach to the YOLO annotator. Both networks use RGB images with the advantage that GoogLeNet runs on an older OpenCV version compatible with ROS.
The Robosherlock implementation uses the trained network to classify objects. The whole class structure is very similar to the YOLO annotator then OpenCV uses the same framework for all supported network architectures. \\
First the pre-trained network get loaded: 
\begin{lstlisting}
.. 
// Load serialized model from disk
net = readNetFromCaffe(prototxtpath, modelPath);
..
// Load class names 
ifstream ifs(classesFile.c_str());
string line;
while(getline(ifs, line)){
   ..
   classes.push_back(resultS);
}
..
\end{lstlisting}
And then the input image get set : 
\begin{lstlisting}
..
// Get Image 
cas.get(VIEW_COLOR_IMAGE, frame);
..
// Create a 4D blob from a frame.
blob = blobFromImage(frame, 1, cvSize(inpWidth, inpHeight), Scalar(104, 117, 123), true, false);
..
// set the blob as input to the network and perform a forward-pass
net.setInput(blob);
result = net.forward();
..
\end{lstlisting}
GoogLeNet differs from YOLO by only outputting the class probability so there is no information about where the classes are in the image. This makes the annotator useless for tracking objects, but can still be used for classification by extracting the classes with high probability.
Due to the output there is no visualization for this annotator.

				\subsection{PythonEmbedding}
\href{https://github.com/SUTURO/suturo_perception/blob/Handcamera_tracking/rs_Athene/src/PythonEmbedding.cpp}{PythonEmbedding} annotator is an example of how a "Python pure embedding" (calling python script during runtime in c++) can be implemented in Robosherlock. The annotator converts the basic image container \texttt{Mat} to NumPy array : 
\begin{lstlisting}
..
//Get image
Mat src;
cas.get(VIEW_COLOR_IMAGE, src);
..
// copy the data from the cv::Mat object into the array
std::memcpy(m, src.data, nElem * sizeof(uchar));

// the dimensions of the matrix
npy_intp mdim[] = { src.rows, src.cols };
    
// convert the cv::Mat to numpy.array
PyObject* mat = PyArray_SimpleNewFromData(2, mdim, NPY_UINT8, (void*) m);
..
\end{lstlisting}

The script “Hello\_World.py” will be loaded :  

\begin{lstlisting}
..
//Load Script
pModule = PyImport_Import(pName);
..
\end{lstlisting}
And the only existing function "start" get called with the created numpy image : 
\begin{lstlisting}
..
//Search for function
pFunc = PyObject_GetAttrString(pModule, "start");
..
//Set variables
pArgs = NULL;
//Call function with Args
pValue = PyObject_CallObject(pFunc, args);
..
\end{lstlisting}
The python script will then open a new window with the send image.\\
The current version of Robosherlock does not support Python annotators, Python pure embedding could represent an interim solution to this problem.

\endgroup
\end{document}

